{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272ca89d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1c78ba-ea05-47f6-85ad-1742973a23a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44407774-3ca6-48cc-b01b-4dae1994edf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba42ef5e-8fc2-4fda-85e6-b657c7a09433",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "HF_TOKEN=\"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\"\n",
    "login(token=HF_TOKEN)\n",
    "\n",
    "# Or via environment variable (safer for PSC scripts)\n",
    "# export HF_TOKEN=your_token_here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f646b8-504c-40d8-a879-66d3b2516035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "# export HF_TOKEN=\"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\"\n",
    "# V100 optimized loading (float16)\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "teacher = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\",cache_dir=\"/ocean/projects/cis220031p/bkoduru/huggingface/hub\")\n",
    "\n",
    "# Map Llama tokens for \"0\"-\"9\" to their indices\n",
    "digit_tokens = [tokenizer.encode(str(i), add_special_tokens=False)[-1] for i in range(10)]\n",
    "\n",
    "def extract_logits(dataloader, limit=2000):\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for i, (img, label) in enumerate(tqdm(dataloader)):\n",
    "        if i >= limit: break\n",
    "        \n",
    "        # Convert 28x28 image to space-separated string\n",
    "        pixels = (img.view(-1) * 255).int().tolist()\n",
    "        pixel_str = \" \".join(map(str, pixels))\n",
    "        prompt = f\"The following is a handwritten digit image pixel list: {pixel_str}\\nDigit:\"\n",
    "        \n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        with torch.no_grad():\n",
    "            outputs = teacher(**inputs)\n",
    "            # Take last token logits and filter for 0-9\n",
    "            logits = outputs.logits[0, -1, digit_tokens] \n",
    "            all_logits.append(logits.cpu())\n",
    "            all_labels.append(label)\n",
    "            \n",
    "    return torch.stack(all_logits), torch.stack(all_labels)\n",
    "\n",
    "# Save for fast training later\n",
    "# mnist_logits, labels = extract_logits(train_loader)\n",
    "# torch.save({'logits': mnist_logits, 'labels': labels}, 'llama3_mnist_logits.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db359b9d-c97e-40f2-a06b-65bdd7bfc21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class StudentMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(784, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x.view(x.size(0), -1))\n",
    "\n",
    "def distillation_loss(student_logits, teacher_logits, labels, T=3.0, alpha=0.5):\n",
    "    # 1. Hard Loss (Ground Truth)\n",
    "    hard_loss = F.cross_entropy(student_logits, labels)\n",
    "    \n",
    "    # 2. Soft Loss (Teacher guidance)\n",
    "    # KL(Teacher || Student)\n",
    "    soft_targets = F.softmax(teacher_logits / T, dim=-1)\n",
    "    soft_student = F.log_softmax(student_logits / T, dim=-1)\n",
    "    soft_loss = F.kl_div(soft_student, soft_targets, reduction='batchmean') * (T**2)\n",
    "    \n",
    "    return alpha * hard_loss + (1 - alpha) * soft_loss\n",
    "\n",
    "# Training Loop\n",
    "def train(model, data, epochs=20):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    teacher_logits = data['logits'].to(\"cuda\")\n",
    "    labels = data['labels'].to(\"cuda\")\n",
    "    mnist_imgs = ... # Original MNIST images\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(mnist_imgs)\n",
    "        loss = distillation_loss(outputs, teacher_logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Epoch {epoch} Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871d66e6-09a8-4adf-958e-ad13652a2320",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming 'teacher' and 'tokenizer' are already loaded in your environment\n",
    "# Map Llama tokens for \"0\"-\"9\" to their indices in the vocab\n",
    "digit_tokens = [tokenizer.encode(str(i), add_special_tokens=False)[-1] for i in range(10)]\n",
    "\n",
    "def precompute_logits(num_samples=2000):\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    loader = DataLoader(mnist_train, batch_size=1, shuffle=False)\n",
    "    \n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "    \n",
    "    print(f\"Extracting knowledge for {num_samples} samples...\")\n",
    "    for i, (img, label) in enumerate(tqdm(loader, total=num_samples)):\n",
    "        if i >= num_samples: break\n",
    "        \n",
    "        # Flatten and stringify image\n",
    "        pixels = (img.view(-1) * 255).int().tolist()\n",
    "        pixel_str = \" \".join(map(str, pixels))\n",
    "        prompt = f\"Pixels: {pixel_str}\\nDigit:\"\n",
    "        \n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        with torch.no_grad():\n",
    "            outputs = teacher(**inputs)\n",
    "            # Filter logits for just the 10 digit tokens at the final position\n",
    "            logits = outputs.logits[0, -1, digit_tokens]\n",
    "            all_logits.append(logits.cpu())\n",
    "            all_labels.append(label)\n",
    "            \n",
    "    torch.save({'logits': torch.stack(all_logits), 'labels': torch.stack(all_labels)}, 'mnist_kd_data.pt')\n",
    "    print(\"Done! Knowledge saved to mnist_kd_data.pt\")\n",
    "\n",
    "precompute_logits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de36aca-f57c-42b9-965f-48d04e5f6bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load your extracted knowledge\n",
    "data = torch.load('mnist_kd_data.pt')\n",
    "t_logits = data['logits']\n",
    "labels = data['labels'].view(-1)\n",
    "\n",
    "# 1. Teacher Accuracy: Does it actually 'see' the digits?\n",
    "t_preds = t_logits.argmax(dim=1)\n",
    "t_acc = (t_preds == labels).float().mean()\n",
    "print(f\"Teacher Accuracy: {t_acc.item():.2%}\")\n",
    "\n",
    "# 2. Logit Entropy: Is there 'Dark Knowledge'?\n",
    "# High entropy means the teacher sees similarities (e.g., a 7 looks like a 1).\n",
    "t_probs = F.softmax(t_logits, dim=-1)\n",
    "entropy = -torch.sum(t_probs * torch.log(t_probs + 1e-9), dim=1).mean()\n",
    "print(f\"Average Teacher Entropy: {entropy.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346f0891-655c-45df-ac19-a8fc3d8a5d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def analyze_teacher_failures(logits, labels, num_examples=5):\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    preds = logits.argmax(dim=1)\n",
    "    \n",
    "    # Find indices where the teacher was WRONG\n",
    "    wrong_indices = (preds != labels).nonzero(as_tuple=True)[0]\n",
    "    \n",
    "    for i in range(min(num_examples, len(wrong_indices))):\n",
    "        idx = wrong_indices[i]\n",
    "        plt.figure(figsize=(6, 2))\n",
    "        plt.bar(range(10), probs[idx].numpy(), color='salmon')\n",
    "        plt.title(f\"True Label: {labels[idx].item()} | Teacher Predicted: {preds[idx].item()}\")\n",
    "        plt.xticks(range(10))\n",
    "        plt.ylabel(\"Probability\")\n",
    "        plt.show()\n",
    "\n",
    "analyze_teacher_failures(t_logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f37414-9f7e-40c3-864c-f27aa5deb10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Load data\n",
    "data = torch.load('mnist_kd_data.pt')\n",
    "logits = data['logits'].numpy()\n",
    "labels = data['labels'].view(-1).numpy()\n",
    "\n",
    "# Run t-SNE on the 10-dimensional logit space\n",
    "tsne = TSNE(n_components=3, perplexity=6, random_state=42)\n",
    "embeddings = tsne.fit_transform(logits)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 7))\n",
    "scatter = plt.scatter(embeddings[:, 0], embeddings[:, 1], c=labels, cmap='tab10', alpha=0.4)\n",
    "plt.colorbar(scatter)\n",
    "plt.title(\"t-SNE Visualization of Llama 3.1 Logits for MNIST\")\n",
    "plt.xlabel(\"t-SNE 1\")\n",
    "plt.ylabel(\"t-SNE 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c9d5c63-0bad-42c3-abe0-0339b803319e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7afc43f1-ac99-4c62-9b0c-75ab0d4c0d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Feb 15 14:41:23 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla V100-SXM2-32GB           On  |   00000000:15:00.0 Off |                    0 |\n",
      "| N/A   29C    P0             41W /  300W |       4MiB /  32768MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ant GPU)",
   "language": "python",
   "name": "ant"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
